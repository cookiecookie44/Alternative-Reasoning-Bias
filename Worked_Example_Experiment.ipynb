{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-omki1ri3ceQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from itertools import combinations\n",
        "import torch.nn.functional as F\n",
        "from matplotlib.colors import ListedColormap\n",
        "import os\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0nH3rNJ_sUY"
      },
      "source": [
        "## Initialisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYrPVBsla44l"
      },
      "outputs": [],
      "source": [
        "def mala_acceptance_probability(x_k: torch.Tensor, x_k_plus_1: torch.Tensor,\n",
        "                               grad_log_pi_x_k: torch.Tensor, grad_log_pi_x_k_plus_1: torch.Tensor,\n",
        "                               log_pi_x_k: torch.Tensor, log_pi_x_k_plus_1: torch.Tensor,\n",
        "                               epsilon: float) -> torch.Tensor:\n",
        "    proposal_mean_forward = x_k + epsilon * grad_log_pi_x_k\n",
        "    diff_forward = x_k_plus_1 - proposal_mean_forward\n",
        "    log_q_forward = -torch.sum(diff_forward ** 2) / (4 * epsilon)\n",
        "\n",
        "    proposal_mean_backward = x_k_plus_1 + epsilon * grad_log_pi_x_k_plus_1\n",
        "    diff_backward = x_k - proposal_mean_backward\n",
        "    log_q_backward = -torch.sum(diff_backward ** 2) / (4 * epsilon)\n",
        "\n",
        "    log_accept_ratio = (log_pi_x_k + log_q_backward - log_pi_x_k_plus_1 - log_q_forward)\n",
        "    return torch.min(torch.ones_like(log_accept_ratio), torch.exp(log_accept_ratio))\n",
        "\n",
        "\n",
        "def av_log_likelihood(model, X, Y, noise_variance=1.0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(X)\n",
        "        log_probs = -0.5 * ((Y - pred)**2 / noise_variance + torch.log(torch.tensor(2 * np.pi * noise_variance)))\n",
        "        return log_probs.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sgld_llc_estimator_behavioral(model, x_data, y_data=None, scale=1.0, step_size=1e-5, sgld_iters=1000, batch_size=32, burn_in=200, diagnostics = False):\n",
        "\n",
        "    dataset_size = len(x_data)\n",
        "    beta_star = 1.0 / np.log(dataset_size)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        f_theta_star = model(x_data).detach()\n",
        "\n",
        "    # Save original weights\n",
        "    w_star = {k: v.clone() for k, v in model.state_dict().items()}\n",
        "    array_neg_behavioral_loss = []\n",
        "    mala_probs = []\n",
        "\n",
        "    for t in range(sgld_iters):\n",
        "        idx = torch.randint(0, dataset_size, (batch_size,))\n",
        "        x_batch = x_data[idx]\n",
        "        f_star_batch = f_theta_star[idx]\n",
        "\n",
        "        model.train()\n",
        "        pred = model(x_batch)\n",
        "        loss_before = ((pred - f_star_batch) ** 2).mean()  # Behavioral loss\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss_before.backward()\n",
        "\n",
        "        grads_before = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "        params_before = torch.cat([p.flatten() for p in model.parameters()])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for name, param in model.named_parameters():\n",
        "                grad = param.grad\n",
        "                eta = torch.randn_like(param) * np.sqrt(step_size)\n",
        "                prior_force = scale * (w_star[name] - param)\n",
        "                delta = (step_size / 2) * (prior_force + dataset_size * beta_star * (-grad)) + eta\n",
        "                param.add_(delta)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_post = model(x_batch)\n",
        "            behav_loss = ((pred_post - f_star_batch) ** 2).mean()\n",
        "            array_neg_behavioral_loss.append(-behav_loss.item())\n",
        "\n",
        "        if t % 20 == 0 and t >= burn_in:\n",
        "            model.train()\n",
        "            pred = model(x_batch)\n",
        "            loss_after = ((pred - f_star_batch) ** 2).mean()\n",
        "            model.zero_grad()\n",
        "            loss_after.backward()\n",
        "\n",
        "            grads_after = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "            params_after = torch.cat([p.flatten() for p in model.parameters()])\n",
        "            w_star_flat = torch.cat([w_star[name].flatten() for name in w_star])\n",
        "\n",
        "            log_pi_before = beta_star * (-dataset_size * loss_before.item()) - 0.5 * scale * torch.sum((params_before - w_star_flat) ** 2)\n",
        "            log_pi_after = beta_star * (-dataset_size * loss_after.item()) - 0.5 * scale * torch.sum((params_after - w_star_flat) ** 2)\n",
        "\n",
        "            mala_prob = mala_acceptance_probability(params_before, params_after, grads_before, grads_after,\n",
        "                                                    log_pi_before, log_pi_after, step_size)\n",
        "            mala_probs.append(mala_prob.item())\n",
        "\n",
        "    array_neg_behavioral_loss = np.array(array_neg_behavioral_loss[burn_in:])\n",
        "    wbic_behavioral = -dataset_size * np.mean(array_neg_behavioral_loss)\n",
        "\n",
        "    model.load_state_dict(w_star)\n",
        "    model.eval()\n",
        "\n",
        "    lambda_hat_behavioral = wbic_behavioral / np.log(dataset_size)\n",
        "\n",
        "    if diagnostics:\n",
        "      if mala_probs:\n",
        "          print(f\"Average MALA acceptance probability: {np.mean(mala_probs):.4f}\")\n",
        "\n",
        "    return lambda_hat_behavioral"
      ],
      "metadata": {
        "id": "P_hCFo4Ea_28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MonotonicLoss:\n",
        "    def __init__(self, model, base_loss=None, lambda_mono=5.0, delta=1e-3):\n",
        "        self.model = model\n",
        "        self.base_loss = base_loss if base_loss is not None else nn.MSELoss()\n",
        "        self.lambda_mono = lambda_mono\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, pred, target, inputs):\n",
        "        base = self.base_loss(pred, target)\n",
        "        mono = self.monotonicity_penalty(inputs)\n",
        "        return base + self.lambda_mono * mono, base\n",
        "\n",
        "    def monotonicity_penalty(self, x):\n",
        "        x = x.clone().detach().requires_grad_(True)\n",
        "        y = self.model(x)\n",
        "        grad = torch.autograd.grad(outputs=y.sum(), inputs=x, create_graph=True)[0]\n",
        "        violations = F.relu(-grad)\n",
        "        return violations.mean()\n",
        "\n",
        "    def grad_stats(self, xb, yb):\n",
        "        \"\"\"Compute base vs penalty gradient dominance metrics for one minibatch.\"\"\"\n",
        "        params = list(self.model.parameters())\n",
        "\n",
        "        pred = self.model(xb)\n",
        "        base_loss = self.base_loss(pred, yb)\n",
        "        mono_pen = self.monotonicity_penalty(xb)\n",
        "\n",
        "        g_base = torch.autograd.grad(base_loss, params, retain_graph=True,\n",
        "                                     create_graph=True, allow_unused=True)\n",
        "        g_pen = torch.autograd.grad(mono_pen, params, retain_graph=True,\n",
        "                                    create_graph=True, allow_unused=True)\n",
        "\n",
        "        g_base = [torch.zeros_like(p) if g is None else g for g,p in zip(g_base,params)]\n",
        "        g_pen  = [torch.zeros_like(p) if g is None else g for g,p in zip(g_pen, params)]\n",
        "\n",
        "        g_base = torch.cat([g.view(-1) for g in g_base])\n",
        "        g_pen  = torch.cat([g.view(-1) for g in g_pen])\n",
        "\n",
        "        # scaled penalty gradient (as it actually enters F)\n",
        "        g_pen_scaled = self.lambda_mono * g_pen\n",
        "        g_total = g_base + g_pen_scaled\n",
        "\n",
        "        Rg = (g_pen_scaled.norm() / (g_base.norm() + 1e-12)).item()\n",
        "        C_lp = torch.nn.functional.cosine_similarity(g_base, g_pen_scaled, dim=0).item()\n",
        "        Dg = (g_pen_scaled @ g_total).item() / ((g_base @ g_total).item() + 1e-12)\n",
        "\n",
        "        return Rg, C_lp, Dg\n",
        "\n",
        "\n",
        "class DeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, loss_fn=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims + [output_dim]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1], bias=True))\n",
        "            if i < len(dims) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        if loss_fn == 'mono':\n",
        "            self.loss_fn = MonotonicLoss(self)\n",
        "        else:\n",
        "            self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def train_model(self, X_train, y_train, epochs=200, lr=1e-2, batch_size=64):\n",
        "\n",
        "        X_test = X_train[:1000]\n",
        "        y_test = y_train[:1000]\n",
        "        X_train = X_train[1000:]\n",
        "        y_train = y_train[1000:]\n",
        "\n",
        "        param_trajectory = []\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        epoch_losses, epoch_base_losses = [], []\n",
        "        converged_point, converged_loss = None, None\n",
        "        previous_loss = float('inf')\n",
        "        cosine_sims, prev_grad, time_elapsed = [], None, []\n",
        "\n",
        "        grad_ratios, grad_cosines, grad_dir_ratios = [], [], []\n",
        "\n",
        "        # initial test loss\n",
        "        self.eval()\n",
        "        pred_init = self.forward(X_test)\n",
        "        if isinstance(self.loss_fn, MonotonicLoss):\n",
        "            _, init_loss = self.loss_fn(pred_init, y_test, X_test)\n",
        "            init_loss = init_loss.item()\n",
        "        else:\n",
        "            init_loss = self.loss_fn(pred_init, y_test).item()\n",
        "        self.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            time_start = time.time()\n",
        "            epoch_loss, epoch_base_loss = 0.0, 0.0\n",
        "\n",
        "            for xb, yb in dataloader:\n",
        "                pred = self(xb)\n",
        "                if isinstance(self.loss_fn, MonotonicLoss):\n",
        "                    loss, base_loss = self.loss_fn(pred, yb, xb)\n",
        "                    epoch_base_loss += base_loss.item() * xb.size(0)\n",
        "                else:\n",
        "                    loss = self.loss_fn(pred, yb)\n",
        "                    epoch_base_loss += loss.item() * xb.size(0)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                curr_grad = torch.cat([p.grad.view(-1) for p in self.parameters() if p.grad is not None])\n",
        "                if prev_grad is not None:\n",
        "                    cos_sim = torch.nn.functional.cosine_similarity(curr_grad, prev_grad, dim=0).item()\n",
        "                    cosine_sims.append(cos_sim)\n",
        "                prev_grad = curr_grad.detach()\n",
        "\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "                if isinstance(self.loss_fn, MonotonicLoss):\n",
        "                    Rg, C_lp, Dg = self.loss_fn.grad_stats(xb, yb)\n",
        "                    grad_ratios.append(Rg)\n",
        "                    grad_cosines.append(C_lp)\n",
        "                    grad_dir_ratios.append(Dg)\n",
        "\n",
        "            avg_loss = epoch_loss / len(dataloader.dataset)\n",
        "            avg_base_loss = epoch_base_loss / len(dataloader.dataset)\n",
        "            epoch_losses.append(avg_loss)\n",
        "            epoch_base_losses.append(avg_base_loss)\n",
        "\n",
        "            if (epoch + 1) % 50 == 0 or epoch < 15:\n",
        "                if isinstance(self.loss_fn, MonotonicLoss):\n",
        "                    print(f\"Epoch {epoch+1:3d} | Total Loss: {avg_loss:.6f} | MSE Loss: {avg_base_loss:.6f}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.6f}\")\n",
        "                param_vector = torch.cat([p.detach().flatten() for p in self.parameters()])\n",
        "                param_trajectory.append(param_vector.clone().cpu())\n",
        "\n",
        "            loss_change = previous_loss - avg_base_loss\n",
        "            if converged_point is None and loss_change < 1e-2:\n",
        "                converged_point, converged_loss = epoch + 1, avg_base_loss\n",
        "                print(f\"Converged at epoch {converged_point}\")\n",
        "            elif converged_point is not None and loss_change >= 1e-2:\n",
        "                converged_point, converged_loss = None, None\n",
        "                print(f\"Convergence broken at epoch {epoch+1}\")\n",
        "            previous_loss = avg_base_loss\n",
        "\n",
        "            time_elapsed.append(time.time() - time_start)\n",
        "\n",
        "        avg_cos_sim = sum(cosine_sims)/len(cosine_sims) if cosine_sims else None\n",
        "\n",
        "        # test evaluation\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_test_pred = self(X_test)\n",
        "            mse_loss_fn = nn.MSELoss()\n",
        "            test_loss_val = mse_loss_fn(y_test_pred, y_test).item()\n",
        "        print(f\"Final test loss on held-out 1000 points: {test_loss_val:.6f}\")\n",
        "\n",
        "        if isinstance(self.loss_fn, MonotonicLoss) and grad_ratios:\n",
        "            print(f\"Avg gradient norm ratio (penalty/base): {sum(grad_ratios)/len(grad_ratios):.3f}\")\n",
        "            print(f\"Avg grad cosine (base vs penalty): {sum(grad_cosines)/len(grad_cosines):.3f}\")\n",
        "            print(f\"Avg directional dominance ratio: {sum(grad_dir_ratios)/len(grad_dir_ratios):.3f}\")\n",
        "\n",
        "        if converged_point:\n",
        "            return init_loss, converged_point, converged_loss, \\\n",
        "                   converged_point * sum(time_elapsed)/len(time_elapsed), \\\n",
        "                   avg_cos_sim, test_loss_val, param_trajectory\n",
        "        else:\n",
        "            return init_loss, None, None, None, None, test_loss_val, param_trajectory\n"
      ],
      "metadata": {
        "id": "objtWyfhpV3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9njGh4tBqCw"
      },
      "outputs": [],
      "source": [
        "class MonotoneLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias = True):\n",
        "        super().__init__()\n",
        "        self.weight_unconstrained = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias_unconstrained = nn.Parameter(torch.randn(out_features))\n",
        "        else:\n",
        "            self.register_parameter(\"bias_unconstrained\", None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = F.softplus(self.weight_unconstrained)\n",
        "        bias = self.bias_unconstrained\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "class MonotoneNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, loss_fn=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if loss_fn is None:\n",
        "            self.loss_fn = nn.MSELoss()\n",
        "        else:\n",
        "            self.loss_fn = loss_fn\n",
        "\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims\n",
        "\n",
        "        for i in range(len(hidden_dims)):\n",
        "            layers.append(MonotoneLinear(dims[i], dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        layers.append(MonotoneLinear(dims[-1], output_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def train_model(self, X_train, y_train, epochs=200, lr=1e-2, batch_size=64):\n",
        "\n",
        "        X_test = X_train[:1000]\n",
        "        y_test = y_train[:1000]\n",
        "        X_train = X_train[1000:]\n",
        "        y_train = y_train[1000:]\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "        loss_fn = self.loss_fn\n",
        "\n",
        "        converged_point = None\n",
        "        converged_loss = None\n",
        "        previous_loss = float('inf')\n",
        "\n",
        "        cosine_sims = []\n",
        "        prev_grad = None\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        epoch_losses = []\n",
        "        time_elapsed = []\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_init = self.forward(X_test)\n",
        "            init_loss = self.loss_fn(pred_init, y_test).item()\n",
        "        self.train()\n",
        "\n",
        "        convergence_broken = False\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            time_start = time.time()\n",
        "            epoch_loss = 0.0\n",
        "            for xb, yb in dataloader:\n",
        "                pred = self(xb)\n",
        "                loss = loss_fn(pred, yb)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                curr_grad = torch.cat([p.grad.view(-1) for p in self.parameters() if p.grad is not None])\n",
        "                if prev_grad is not None:\n",
        "                    cos_sim = torch.nn.functional.cosine_similarity(curr_grad, prev_grad, dim=0).item()\n",
        "                    cosine_sims.append(cos_sim)\n",
        "                prev_grad = curr_grad.detach()\n",
        "\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "            avg_loss = epoch_loss / len(dataloader.dataset)\n",
        "            epoch_losses.append(avg_loss)\n",
        "\n",
        "            if (epoch + 1) % 50 == 0 or epoch < 15:\n",
        "                print(f\"Epoch {epoch + 1:3d} | Loss: {avg_loss:.6f}\")\n",
        "\n",
        "            loss_change = previous_loss - avg_loss\n",
        "\n",
        "            if converged_point is None:\n",
        "                if loss_change < 1e-2:\n",
        "                    converged_point = epoch + 1\n",
        "                    converged_loss = avg_loss\n",
        "                    print(f\"Converged at epoch {converged_point}\")\n",
        "            else:\n",
        "                if loss_change >= 1e-2:\n",
        "                    converged_point = None\n",
        "                    converged_loss = None\n",
        "                    convergence_broken = True\n",
        "                    print(f\"Convergence broken at epoch {epoch + 1}\")\n",
        "\n",
        "            previous_loss = avg_loss\n",
        "            time_end = time.time()\n",
        "            time_elapsed.append(time_end - time_start)\n",
        "\n",
        "        if converged_point is not None:\n",
        "            total_steps = len(cosine_sims)\n",
        "            steps_per_epoch = total_steps // epochs\n",
        "            cutoff = steps_per_epoch * converged_point\n",
        "            cosine_sims = cosine_sims[:cutoff]\n",
        "\n",
        "        avg_cos_sim = sum(cosine_sims) / len(cosine_sims) if cosine_sims else None\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_test_pred = self(X_test)\n",
        "            mse_loss_fn = nn.MSELoss()\n",
        "            test_loss_val = mse_loss_fn(y_test_pred, y_test).item()\n",
        "\n",
        "        print(f\"Final test loss on held-out 1000 points: {test_loss_val:.6f}\")\n",
        "\n",
        "        if converged_point:\n",
        "          return init_loss, converged_point, converged_loss, converged_point * sum(time_elapsed)/len(time_elapsed), avg_cos_sim, test_loss_val\n",
        "        else:\n",
        "          return init_loss, None, None, None, None, test_loss_val"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation Code"
      ],
      "metadata": {
        "id": "Sjl4P2DTbmfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr4BHLmKcoda"
      },
      "outputs": [],
      "source": [
        "def visualise_2d_loss_grid(x_data, y_data, nn, param_range=2.0, resolution=50,\n",
        "                           trajectory=None, savepath='monotonic.png'):\n",
        "\n",
        "    params = list(nn.parameters())\n",
        "    original_params = [p.clone() for p in params]\n",
        "    original_vector = torch.cat([p.flatten() for p in params])\n",
        "    n_params = len(original_vector)\n",
        "\n",
        "    fig, axes = plt.subplots(n_params, n_params, figsize=(3*n_params, 3*n_params))\n",
        "    if n_params == 1:\n",
        "        axes = [[axes]]\n",
        "    elif n_params == 2:\n",
        "        axes = axes.reshape(2, 2)\n",
        "\n",
        "    nn.eval()\n",
        "\n",
        "    for i in range(n_params):\n",
        "        for j in range(n_params):\n",
        "            ax = axes[i][j]\n",
        "\n",
        "            if i == j:\n",
        "                # 1D slice\n",
        "                orig_val = original_vector[i].item()\n",
        "                param_vals = np.linspace(orig_val - param_range, orig_val + param_range, resolution)\n",
        "                losses = []\n",
        "\n",
        "                for val in param_vals:\n",
        "                    current_vector = original_vector.clone()\n",
        "                    current_vector[i] = val\n",
        "                    _set_parameters_from_vector(nn, params, current_vector)\n",
        "                    pred = nn(x_data)\n",
        "                    if isinstance(nn.loss_fn, MonotonicLoss):\n",
        "                        _, loss = nn.loss_fn(pred, y_data, x_data)\n",
        "                    else:\n",
        "                        loss = nn.loss_fn(pred, y_data)\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                ax.plot(param_vals, losses, 'b-', linewidth=2)\n",
        "                ax.axvline(orig_val, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "                # ✅ overlay trajectory points\n",
        "                if trajectory is not None:\n",
        "                    traj_vals = [vec[i].item() for vec in trajectory]\n",
        "                    ax.plot(traj_vals, [None]*len(traj_vals), 'ko--', markersize=4)\n",
        "\n",
        "                ax.set_xlabel(f\"W[{i}]\")\n",
        "                ax.set_ylabel('Loss')\n",
        "                ax.set_title(f'Param {i} slice')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            else:\n",
        "                # 2D slice\n",
        "                pi_orig = original_vector[i].item()\n",
        "                pj_orig = original_vector[j].item()\n",
        "                pi_range = np.linspace(pi_orig - param_range, pi_orig + param_range, resolution)\n",
        "                pj_range = np.linspace(pj_orig - param_range, pj_orig + param_range, resolution)\n",
        "                Pi, Pj = np.meshgrid(pi_range, pj_range)\n",
        "                loss_surface = np.zeros_like(Pi)\n",
        "\n",
        "                for ii in range(resolution):\n",
        "                    for jj in range(resolution):\n",
        "                        current_vector = original_vector.clone()\n",
        "                        current_vector[i] = Pi[ii, jj]\n",
        "                        current_vector[j] = Pj[ii, jj]\n",
        "                        _set_parameters_from_vector(nn, params, current_vector)\n",
        "                        pred = nn(x_data)\n",
        "                        if isinstance(nn.loss_fn, MonotonicLoss):\n",
        "                            loss, _ = nn.loss_fn(pred, y_data, x_data)\n",
        "                        else:\n",
        "                            loss = nn.loss_fn(pred, y_data)\n",
        "                        loss_surface[ii, jj] = loss.item()\n",
        "\n",
        "                contourf_plot = ax.contourf(Pi, Pj, loss_surface, levels=12, cmap='viridis', alpha=0.6)\n",
        "                ax.contour(Pi, Pj, loss_surface, levels=12, cmap='viridis')\n",
        "                ax.plot(pi_orig, pj_orig, 'ro', markersize=4)\n",
        "\n",
        "                # ✅ overlay trajectory\n",
        "                if trajectory is not None:\n",
        "                    traj_pi = [vec[i].item() for vec in trajectory]\n",
        "                    traj_pj = [vec[j].item() for vec in trajectory]\n",
        "                    ax.plot(traj_pi, traj_pj, 'ko--', markersize=4)\n",
        "\n",
        "                plt.colorbar(contourf_plot, ax=ax, shrink=0.8).set_label('Loss', rotation=270, labelpad=15)\n",
        "\n",
        "                ax.set_xlabel(f\"W[{i}]\")\n",
        "                ax.set_ylabel(f\"W[{j}]\")\n",
        "                ax.set_title(f\"W[{i}] vs W[{j}]\")\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "    for param, orig in zip(params, original_params):\n",
        "        param.data.copy_(orig.data)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(savepath, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _set_parameters_from_vector(nn, params, param_vector):\n",
        "    \"\"\"Helper function to set network parameters from a flattened vector.\"\"\"\n",
        "    start_idx = 0\n",
        "    for param in params:\n",
        "        param_size = param.numel()\n",
        "        param.data = param_vector[start_idx:start_idx + param_size].reshape(param.shape)\n",
        "        start_idx += param_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_monotonicity_violation(nn, x_data):\n",
        "    \"\"\"\n",
        "    Check if current network parameters violate monotonicity constraints.\n",
        "    Returns the total violation (0 means feasible).\n",
        "    - Uses PURELY the weights, no loss function involved.\n",
        "    \"\"\"\n",
        "    nn.eval()\n",
        "\n",
        "    x_data = x_data.clone().detach().requires_grad_(True)\n",
        "    y_pred = nn(x_data)\n",
        "\n",
        "    grad = torch.autograd.grad(\n",
        "        outputs=y_pred.sum(),\n",
        "        inputs=x_data,\n",
        "        create_graph=False,\n",
        "        retain_graph=False\n",
        "    )[0]\n",
        "\n",
        "    violations = F.relu(-grad)\n",
        "    return violations.mean()"
      ],
      "metadata": {
        "id": "teZwV-Yl2F7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXJgZaNpU1D6"
      },
      "outputs": [],
      "source": [
        "def visualise_feasible_region_mono(x_data, y_data, nn, param_range=2.0, resolution=50,\n",
        "                                  violation_threshold=0, savepath='constraint.png'):\n",
        "    \"\"\"\n",
        "    Visualize the feasible region where monotonicity constraints are satisfied,\n",
        "    overlaid with the loss landscape.\n",
        "    - Feasible region (green/red) depends ONLY on weights (gradient check).\n",
        "    - Loss contours depend on the model's loss function (MSE vs. MonotonicLoss).\n",
        "    \"\"\"\n",
        "    params = list(nn.parameters())\n",
        "    param_names = [f\"W{i}[{j},{k}]\" if len(p.shape) > 1 else f\"W{i}[{j}]\"\n",
        "                   for i, p in enumerate(params) for j in range(p.shape[0])\n",
        "                   for k in (range(p.shape[1]) if len(p.shape) > 1 else [0])]\n",
        "\n",
        "    original_params = [p.clone() for p in params]\n",
        "    original_vector = torch.cat([p.flatten() for p in params])\n",
        "    n_params = len(original_vector)\n",
        "\n",
        "    fig, axes = plt.subplots(n_params, n_params, figsize=(4*n_params, 4*n_params))\n",
        "    if n_params == 1:\n",
        "        axes = [[axes]]\n",
        "    elif n_params == 2:\n",
        "        axes = axes.reshape(2, 2)\n",
        "\n",
        "    nn.eval()\n",
        "\n",
        "    for i in range(n_params):\n",
        "        for j in range(n_params):\n",
        "            ax = axes[i][j]\n",
        "\n",
        "            if i == j:\n",
        "                # 1D slice: Plot loss curve with feasible regions colored\n",
        "                orig_val = original_vector[i].item()\n",
        "                param_vals = np.linspace(orig_val - param_range, orig_val + param_range, resolution)\n",
        "                losses = []\n",
        "                violations = []\n",
        "\n",
        "                for val in param_vals:\n",
        "                    current_vector = original_vector.clone()\n",
        "                    current_vector[i] = val\n",
        "                    _set_parameters_from_vector(nn, params, current_vector)\n",
        "\n",
        "                    # Compute loss (depends on model's loss function)\n",
        "                    pred = nn(x_data)\n",
        "                    if isinstance(nn.loss_fn, MonotonicLoss):\n",
        "                        _, loss = nn.loss_fn(pred, y_data, x_data)\n",
        "                    else:\n",
        "                        loss = nn.loss_fn(pred, y_data)\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                    # Compute violation (independent of loss function)\n",
        "                    violation = check_monotonicity_violation(nn, x_data)\n",
        "                    violations.append(violation.item())\n",
        "\n",
        "                # Color segments based on feasibility\n",
        "                for k in range(len(param_vals) - 1):\n",
        "                    color = 'green' if violations[k] <= violation_threshold else 'red'\n",
        "                    ax.plot([param_vals[k], param_vals[k + 1]], [losses[k], losses[k + 1]],\n",
        "                           color=color, linewidth=3, alpha=0.7)\n",
        "\n",
        "                ax.axvline(orig_val, color='blue', linestyle='--', alpha=0.7, label='Original')\n",
        "                ax.set_xlabel(param_names[i])\n",
        "                ax.set_ylabel('Loss')\n",
        "                ax.set_title(f'{param_names[i]} slice\\n(Green=Feasible, Red=Infeasible)')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.legend()\n",
        "\n",
        "            else:\n",
        "                # 2D slice: Plot loss contours with feasible overlay\n",
        "                pi_orig = original_vector[i].item()\n",
        "                pj_orig = original_vector[j].item()\n",
        "\n",
        "                pi_range = np.linspace(pi_orig - param_range, pi_orig + param_range, resolution)\n",
        "                pj_range = np.linspace(pj_orig - param_range, pj_orig + param_range, resolution)\n",
        "                Pi, Pj = np.meshgrid(pi_range, pj_range)\n",
        "\n",
        "                loss_surface = np.zeros_like(Pi)\n",
        "                feasible_mask = np.zeros_like(Pi, dtype=bool)\n",
        "\n",
        "                for ii in range(resolution):\n",
        "                    for jj in range(resolution):\n",
        "                        current_vector = original_vector.clone()\n",
        "                        current_vector[i] = Pi[ii, jj]\n",
        "                        current_vector[j] = Pj[ii, jj]\n",
        "                        _set_parameters_from_vector(nn, params, current_vector)\n",
        "\n",
        "                        # Compute loss (model-specific)\n",
        "                        pred = nn(x_data)\n",
        "                        if isinstance(nn.loss_fn, MonotonicLoss):\n",
        "                            loss, _ = nn.loss_fn(pred, y_data, x_data)\n",
        "                        else:\n",
        "                            loss = nn.loss_fn(pred, y_data)\n",
        "                        loss_surface[ii, jj] = loss.item()\n",
        "\n",
        "                        # Compute feasibility (weight-based only)\n",
        "                        violation = check_monotonicity_violation(nn, x_data)\n",
        "                        feasible_mask[ii, jj] = violation <= violation_threshold\n",
        "\n",
        "                # Plot loss contours (model-specific)\n",
        "                contour = ax.contour(Pi, Pj, loss_surface, levels=12, colors='gray', alpha=0.5)\n",
        "                ax.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "                # Overlay feasible region (weight-based only)\n",
        "                feasible_colored = np.where(feasible_mask, 1, 0)\n",
        "                cmap = ListedColormap(['red', 'lightgreen'])\n",
        "                ax.contourf(Pi, Pj, feasible_colored, levels=[0, 0.5, 1], cmap=cmap, alpha=0.4)\n",
        "\n",
        "                # Mark original point\n",
        "                _set_parameters_from_vector(nn, params, original_vector)\n",
        "                original_violation = check_monotonicity_violation(nn, x_data)\n",
        "                original_color = 'blue' if original_violation <= violation_threshold else 'orange'\n",
        "                ax.plot(pi_orig, pj_orig, 'o', color=original_color, markersize=8,\n",
        "                        label=f'Original ({\"Feasible\" if original_violation <= violation_threshold else \"Infeasible\"})')\n",
        "\n",
        "                ax.set_xlabel(param_names[i])\n",
        "                ax.set_ylabel(param_names[j])\n",
        "                ax.set_title(f'{param_names[i]} vs {param_names[j]}\\n(Green=Feasible, Red=Infeasible)')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.legend()\n",
        "\n",
        "    # Restore original parameters\n",
        "    for param, orig in zip(params, original_params):\n",
        "        param.data.copy_(orig.data)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(savepath, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z7LkLwi_p8d"
      },
      "source": [
        "## Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_O6INVLynQQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "np.random.seed(10)\n",
        "random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_weights(model):\n",
        "    return [param.detach().clone() for param in model.parameters()]"
      ],
      "metadata": {
        "id": "pQt05XtL2Mxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# store metrics\n",
        "base_losses, loss_losses, arch_losses = [], [], []\n",
        "\n",
        "init_weights_all = []\n",
        "final_weights_base = []\n",
        "final_weights_loss = []\n",
        "final_weights_arch = []\n",
        "\n",
        "base_distances = []\n",
        "loss_distances = []\n",
        "arch_distances = []\n",
        "\n",
        "def clone_monotone_weights(model):\n",
        "    \"\"\"Clones the unconstrained weights for MonotoneNetwork.\"\"\"\n",
        "    weights = []\n",
        "    for layer in model.model:\n",
        "        if isinstance(layer, MonotoneLinear):\n",
        "            weights.append(layer.weight_unconstrained.detach().clone())\n",
        "            if layer.bias_unconstrained is not None:\n",
        "                 weights.append(layer.bias_unconstrained.detach().clone())\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "             weights.append(layer.weight.detach().clone())\n",
        "             if layer.bias is not None:\n",
        "                  weights.append(layer.bias.detach().clone())\n",
        "    return weights\n",
        "\n",
        "\n",
        "for run in range(10):\n",
        "    # Generate synthetic data\n",
        "    example_mono_x = torch.randn(2000, 2) * 4\n",
        "    example_mono_x = F.softplus(example_mono_x)\n",
        "\n",
        "    A, alpha, beta = 1.0, 0.6, 0.4\n",
        "    example_mono_y = A * (example_mono_x[:, 0] ** alpha) * (example_mono_x[:, 1] ** beta)\n",
        "    example_mono_y = example_mono_y.unsqueeze(1)\n",
        "\n",
        "    # Models\n",
        "    baseline_nn = DeepNeuralNetwork(2, [2], 1)\n",
        "    loss_nn = DeepNeuralNetwork(2, [2], 1, loss_fn='mono')\n",
        "    arch_nn = MonotoneNetwork(2, [2], 1)\n",
        "\n",
        "    # Clone initial weights (using the standard clone_weights for DeepNeuralNetwork)\n",
        "    init_weights_base = clone_weights(baseline_nn)\n",
        "    init_weights_loss = clone_weights(loss_nn)\n",
        "    # Clone initial weights for MonotoneNetwork\n",
        "    init_weights_arch = clone_monotone_weights(arch_nn)\n",
        "\n",
        "\n",
        "    # Copy initial weights into models\n",
        "    with torch.no_grad():\n",
        "        for p, w in zip(baseline_nn.parameters(), init_weights_base):\n",
        "            p.copy_(w)\n",
        "        for p, w in zip(loss_nn.parameters(), init_weights_loss):\n",
        "            p.copy_(w)\n",
        "\n",
        "        # Copy initial weights into MonotoneNetwork (unconstrained weights)\n",
        "        i = 0\n",
        "        for layer in arch_nn.model:\n",
        "            if isinstance(layer, MonotoneLinear):\n",
        "                layer.weight_unconstrained.copy_(init_weights_arch[i])\n",
        "                i += 1\n",
        "                if layer.bias_unconstrained is not None:\n",
        "                    layer.bias_unconstrained.copy_(init_weights_arch[i])\n",
        "                    i += 1\n",
        "\n",
        "\n",
        "    # Train\n",
        "    base_losses.append(baseline_nn.train_model(example_mono_x, example_mono_y))\n",
        "    loss_losses.append(loss_nn.train_model(example_mono_x, example_mono_y))\n",
        "    arch_losses.append(arch_nn.train_model(example_mono_x, example_mono_y))\n",
        "\n",
        "    # Final weights\n",
        "    final_w_base = clone_weights(baseline_nn)\n",
        "    final_w_loss = clone_weights(loss_nn)\n",
        "    final_w_arch = clone_monotone_weights(arch_nn)\n",
        "\n",
        "    final_weights_base.append(final_w_base)\n",
        "    final_weights_loss.append(final_w_loss)\n",
        "    final_weights_arch.append(final_w_arch)\n",
        "\n",
        "\n",
        "    # Distances (squared difference norm) - Flatten and concatenate for comparison\n",
        "    base_diff_sq = [(f - i) ** 2 for f, i in zip(final_w_base, init_weights_base)]\n",
        "    loss_diff_sq = [(f - i) ** 2 for f, i in zip(final_w_loss, init_weights_loss)]\n",
        "    arch_diff_sq = [(f - i) ** 2 for f, i in zip(final_w_arch, init_weights_arch)]\n",
        "\n",
        "\n",
        "    base_distances.append(torch.mean(torch.cat([d.flatten() for d in base_diff_sq])))\n",
        "    loss_distances.append(torch.mean(torch.cat([d.flatten() for d in loss_diff_sq])))\n",
        "    arch_distances.append(torch.mean(torch.cat([d.flatten() for d in arch_diff_sq])))\n",
        "\n",
        "    # Store all initial weights for averaging\n",
        "    init_weights_all.append(torch.cat([w.flatten() for w in init_weights_base])) # Use base as representative\n",
        "\n",
        "\n",
        "# === Summary statistics ===\n",
        "print(\"Average initial weight value:\", torch.mean(torch.cat(init_weights_all)))\n",
        "print(\"Average final weight (Base):\", torch.mean(torch.cat([torch.cat([w.flatten() for w in weights]) for weights in final_weights_base])))\n",
        "print(\"Average final weight (Loss):\", torch.mean(torch.cat([torch.cat([w.flatten() for w in weights]) for weights in final_weights_loss])))\n",
        "print(\"Average final weight (Arch):\", torch.mean(torch.cat([torch.cat([w.flatten() for w in weights]) for weights in final_weights_arch])))\n",
        "\n",
        "print(\"Average distance (Base):\", torch.mean(torch.stack(base_distances)))\n",
        "print(\"Average distance (Loss):\", torch.mean(torch.stack(loss_distances)))\n",
        "print(\"Average distance (Arch):\", torch.mean(torch.stack(arch_distances)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAlHQGLisrLO",
        "outputId": "e13e96d0-e0f5-4cd5-e39b-badb9341c094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 1.871620\n",
            "Epoch   2 | Loss: 0.842809\n",
            "Epoch   3 | Loss: 0.724643\n",
            "Epoch   4 | Loss: 0.654878\n",
            "Epoch   5 | Loss: 0.644125\n",
            "Epoch   6 | Loss: 0.634808\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Loss: 0.638245\n",
            "Epoch   8 | Loss: 0.639710\n",
            "Epoch   9 | Loss: 0.637361\n",
            "Epoch  10 | Loss: 0.635154\n",
            "Epoch  11 | Loss: 0.634940\n",
            "Epoch  12 | Loss: 0.637070\n",
            "Epoch  13 | Loss: 0.636958\n",
            "Epoch  14 | Loss: 0.636602\n",
            "Epoch  15 | Loss: 0.638195\n",
            "Convergence broken at epoch 42\n",
            "Converged at epoch 43\n",
            "Epoch  50 | Loss: 0.638882\n",
            "Convergence broken at epoch 55\n",
            "Converged at epoch 56\n",
            "Convergence broken at epoch 59\n",
            "Converged at epoch 60\n",
            "Convergence broken at epoch 64\n",
            "Converged at epoch 65\n",
            "Convergence broken at epoch 66\n",
            "Converged at epoch 67\n",
            "Convergence broken at epoch 78\n",
            "Converged at epoch 79\n",
            "Convergence broken at epoch 91\n",
            "Converged at epoch 92\n",
            "Epoch 100 | Loss: 0.639124\n",
            "Convergence broken at epoch 149\n",
            "Epoch 150 | Loss: 0.638067\n",
            "Converged at epoch 150\n",
            "Convergence broken at epoch 187\n",
            "Converged at epoch 188\n",
            "Epoch 200 | Loss: 0.633902\n",
            "Final test loss on held-out 1000 points: 0.595011\n",
            "Epoch   1 | Total Loss: 0.786750 | MSE Loss: 0.786750\n",
            "Epoch   2 | Total Loss: 0.646656 | MSE Loss: 0.646656\n",
            "Epoch   3 | Total Loss: 0.637115 | MSE Loss: 0.637115\n",
            "Converged at epoch 3\n",
            "Epoch   4 | Total Loss: 0.625402 | MSE Loss: 0.625402\n",
            "Convergence broken at epoch 4\n",
            "Epoch   5 | Total Loss: 0.621070 | MSE Loss: 0.621070\n",
            "Converged at epoch 5\n",
            "Epoch   6 | Total Loss: 0.614373 | MSE Loss: 0.614373\n",
            "Epoch   7 | Total Loss: 0.609284 | MSE Loss: 0.609284\n",
            "Epoch   8 | Total Loss: 0.605399 | MSE Loss: 0.605399\n",
            "Epoch   9 | Total Loss: 0.581887 | MSE Loss: 0.581887\n",
            "Convergence broken at epoch 9\n",
            "Epoch  10 | Total Loss: 0.585019 | MSE Loss: 0.585019\n",
            "Converged at epoch 10\n",
            "Epoch  11 | Total Loss: 0.561107 | MSE Loss: 0.561107\n",
            "Convergence broken at epoch 11\n",
            "Epoch  12 | Total Loss: 0.548772 | MSE Loss: 0.548772\n",
            "Epoch  13 | Total Loss: 0.528169 | MSE Loss: 0.528169\n",
            "Epoch  14 | Total Loss: 0.509846 | MSE Loss: 0.509846\n",
            "Epoch  15 | Total Loss: 0.490635 | MSE Loss: 0.490635\n",
            "Converged at epoch 30\n",
            "Epoch  50 | Total Loss: 0.127199 | MSE Loss: 0.127199\n",
            "Epoch 100 | Total Loss: 0.115092 | MSE Loss: 0.109674\n",
            "Epoch 150 | Total Loss: 0.126204 | MSE Loss: 0.104783\n",
            "Epoch 200 | Total Loss: 0.206857 | MSE Loss: 0.111880\n",
            "Final test loss on held-out 1000 points: 0.106059\n",
            "Avg gradient norm ratio (penalty/base): 0.072\n",
            "Avg grad cosine (base vs penalty): -0.040\n",
            "Avg directional dominance ratio: 0.020\n",
            "Epoch   1 | Loss: 4.551684\n",
            "Epoch   2 | Loss: 1.126151\n",
            "Epoch   3 | Loss: 0.951203\n",
            "Epoch   4 | Loss: 0.852802\n",
            "Epoch   5 | Loss: 0.785528\n",
            "Epoch   6 | Loss: 0.738621\n",
            "Epoch   7 | Loss: 0.705063\n",
            "Epoch   8 | Loss: 0.680749\n",
            "Epoch   9 | Loss: 0.663570\n",
            "Epoch  10 | Loss: 0.650533\n",
            "Epoch  11 | Loss: 0.640565\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Loss: 0.633958\n",
            "Epoch  13 | Loss: 0.628209\n",
            "Epoch  14 | Loss: 0.624080\n",
            "Epoch  15 | Loss: 0.621252\n",
            "Epoch  50 | Loss: 0.610886\n",
            "Epoch 100 | Loss: 0.608628\n",
            "Epoch 150 | Loss: 0.607853\n",
            "Epoch 200 | Loss: 0.606455\n",
            "Final test loss on held-out 1000 points: 0.563735\n",
            "Epoch   1 | Loss: 1.394696\n",
            "Epoch   2 | Loss: 1.021833\n",
            "Epoch   3 | Loss: 0.776443\n",
            "Epoch   4 | Loss: 0.632384\n",
            "Epoch   5 | Loss: 0.537927\n",
            "Epoch   6 | Loss: 0.467173\n",
            "Epoch   7 | Loss: 0.413681\n",
            "Epoch   8 | Loss: 0.361176\n",
            "Epoch   9 | Loss: 0.321312\n",
            "Epoch  10 | Loss: 0.283672\n",
            "Epoch  11 | Loss: 0.254301\n",
            "Epoch  12 | Loss: 0.227329\n",
            "Epoch  13 | Loss: 0.204798\n",
            "Epoch  14 | Loss: 0.183476\n",
            "Epoch  15 | Loss: 0.169473\n",
            "Converged at epoch 17\n",
            "Epoch  50 | Loss: 0.095508\n",
            "Epoch 100 | Loss: 0.085784\n",
            "Epoch 150 | Loss: 0.084856\n",
            "Epoch 200 | Loss: 0.084652\n",
            "Final test loss on held-out 1000 points: 0.104887\n",
            "Epoch   1 | Total Loss: 1.629478 | MSE Loss: 1.555023\n",
            "Epoch   2 | Total Loss: 0.825226 | MSE Loss: 0.825226\n",
            "Epoch   3 | Total Loss: 0.744959 | MSE Loss: 0.744959\n",
            "Epoch   4 | Total Loss: 0.692996 | MSE Loss: 0.692996\n",
            "Epoch   5 | Total Loss: 0.658781 | MSE Loss: 0.658781\n",
            "Epoch   6 | Total Loss: 0.626769 | MSE Loss: 0.626769\n",
            "Epoch   7 | Total Loss: 0.608128 | MSE Loss: 0.608128\n",
            "Epoch   8 | Total Loss: 0.596773 | MSE Loss: 0.596773\n",
            "Epoch   9 | Total Loss: 0.586717 | MSE Loss: 0.586717\n",
            "Epoch  10 | Total Loss: 0.577763 | MSE Loss: 0.577763\n",
            "Converged at epoch 10\n",
            "Epoch  11 | Total Loss: 0.575019 | MSE Loss: 0.575019\n",
            "Epoch  12 | Total Loss: 0.578408 | MSE Loss: 0.578408\n",
            "Epoch  13 | Total Loss: 0.571512 | MSE Loss: 0.571512\n",
            "Epoch  14 | Total Loss: 0.568514 | MSE Loss: 0.568514\n",
            "Epoch  15 | Total Loss: 0.561253 | MSE Loss: 0.561253\n",
            "Epoch  50 | Total Loss: 0.556626 | MSE Loss: 0.556626\n",
            "Epoch 100 | Total Loss: 0.552935 | MSE Loss: 0.552935\n",
            "Convergence broken at epoch 132\n",
            "Converged at epoch 133\n",
            "Convergence broken at epoch 141\n",
            "Converged at epoch 142\n",
            "Epoch 150 | Total Loss: 0.531392 | MSE Loss: 0.531392\n",
            "Convergence broken at epoch 150\n",
            "Converged at epoch 151\n",
            "Convergence broken at epoch 154\n",
            "Converged at epoch 155\n",
            "Convergence broken at epoch 180\n",
            "Converged at epoch 181\n",
            "Epoch 200 | Total Loss: 0.528702 | MSE Loss: 0.528702\n",
            "Final test loss on held-out 1000 points: 0.545345\n",
            "Avg gradient norm ratio (penalty/base): 0.000\n",
            "Avg grad cosine (base vs penalty): 0.001\n",
            "Avg directional dominance ratio: 0.000\n",
            "Epoch   1 | Loss: 1.182602\n",
            "Epoch   2 | Loss: 0.977889\n",
            "Epoch   3 | Loss: 0.922689\n",
            "Epoch   4 | Loss: 0.896593\n",
            "Epoch   5 | Loss: 0.880352\n",
            "Epoch   6 | Loss: 0.865580\n",
            "Epoch   7 | Loss: 0.851570\n",
            "Epoch   8 | Loss: 0.837830\n",
            "Epoch   9 | Loss: 0.824531\n",
            "Epoch  10 | Loss: 0.809763\n",
            "Epoch  11 | Loss: 0.796113\n",
            "Epoch  12 | Loss: 0.782098\n",
            "Epoch  13 | Loss: 0.768487\n",
            "Epoch  14 | Loss: 0.754889\n",
            "Epoch  15 | Loss: 0.742960\n",
            "Converged at epoch 22\n",
            "Epoch  50 | Loss: 0.570655\n",
            "Epoch 100 | Loss: 0.564538\n",
            "Epoch 150 | Loss: 0.560759\n",
            "Epoch 200 | Loss: 0.558590\n",
            "Final test loss on held-out 1000 points: 0.603866\n",
            "Epoch   1 | Loss: 2.587609\n",
            "Epoch   2 | Loss: 0.664972\n",
            "Epoch   3 | Loss: 0.634673\n",
            "Epoch   4 | Loss: 0.621350\n",
            "Epoch   5 | Loss: 0.612151\n",
            "Converged at epoch 5\n",
            "Epoch   6 | Loss: 0.599489\n",
            "Convergence broken at epoch 6\n",
            "Epoch   7 | Loss: 0.592858\n",
            "Converged at epoch 7\n",
            "Epoch   8 | Loss: 0.579499\n",
            "Convergence broken at epoch 8\n",
            "Epoch   9 | Loss: 0.559282\n",
            "Epoch  10 | Loss: 0.537028\n",
            "Epoch  11 | Loss: 0.514568\n",
            "Epoch  12 | Loss: 0.486977\n",
            "Epoch  13 | Loss: 0.462989\n",
            "Epoch  14 | Loss: 0.425385\n",
            "Epoch  15 | Loss: 0.393522\n",
            "Converged at epoch 25\n",
            "Convergence broken at epoch 26\n",
            "Converged at epoch 27\n",
            "Epoch  50 | Loss: 0.096566\n",
            "Epoch 100 | Loss: 0.094702\n",
            "Epoch 150 | Loss: 0.094621\n",
            "Epoch 200 | Loss: 0.093237\n",
            "Final test loss on held-out 1000 points: 0.138952\n",
            "Epoch   1 | Total Loss: 3.574964 | MSE Loss: 3.213353\n",
            "Epoch   2 | Total Loss: 2.533696 | MSE Loss: 2.355030\n",
            "Epoch   3 | Total Loss: 1.506813 | MSE Loss: 1.502583\n",
            "Epoch   4 | Total Loss: 1.125770 | MSE Loss: 1.125770\n",
            "Epoch   5 | Total Loss: 0.919860 | MSE Loss: 0.919860\n",
            "Epoch   6 | Total Loss: 0.810806 | MSE Loss: 0.810806\n",
            "Epoch   7 | Total Loss: 0.745317 | MSE Loss: 0.745317\n",
            "Epoch   8 | Total Loss: 0.705905 | MSE Loss: 0.705905\n",
            "Epoch   9 | Total Loss: 0.687680 | MSE Loss: 0.687680\n",
            "Epoch  10 | Total Loss: 0.679759 | MSE Loss: 0.679759\n",
            "Converged at epoch 10\n",
            "Epoch  11 | Total Loss: 0.669846 | MSE Loss: 0.669846\n",
            "Epoch  12 | Total Loss: 0.667054 | MSE Loss: 0.667054\n",
            "Epoch  13 | Total Loss: 0.664932 | MSE Loss: 0.664932\n",
            "Epoch  14 | Total Loss: 0.663445 | MSE Loss: 0.663445\n",
            "Epoch  15 | Total Loss: 0.663633 | MSE Loss: 0.663633\n",
            "Convergence broken at epoch 16\n",
            "Converged at epoch 17\n",
            "Convergence broken at epoch 37\n",
            "Converged at epoch 38\n",
            "Epoch  50 | Total Loss: 0.653665 | MSE Loss: 0.653665\n",
            "Convergence broken at epoch 56\n",
            "Converged at epoch 57\n",
            "Epoch 100 | Total Loss: 0.652791 | MSE Loss: 0.652791\n",
            "Convergence broken at epoch 129\n",
            "Converged at epoch 130\n",
            "Epoch 150 | Total Loss: 0.651102 | MSE Loss: 0.651102\n",
            "Convergence broken at epoch 155\n",
            "Converged at epoch 156\n",
            "Convergence broken at epoch 162\n",
            "Converged at epoch 163\n",
            "Convergence broken at epoch 166\n",
            "Converged at epoch 167\n",
            "Epoch 200 | Total Loss: 0.654141 | MSE Loss: 0.654141\n",
            "Final test loss on held-out 1000 points: 0.671938\n",
            "Avg gradient norm ratio (penalty/base): 0.005\n",
            "Avg grad cosine (base vs penalty): 0.003\n",
            "Avg directional dominance ratio: 0.003\n",
            "Epoch   1 | Loss: 0.846056\n",
            "Epoch   2 | Loss: 0.744635\n",
            "Epoch   3 | Loss: 0.710713\n",
            "Epoch   4 | Loss: 0.694929\n",
            "Epoch   5 | Loss: 0.684953\n",
            "Converged at epoch 5\n",
            "Epoch   6 | Loss: 0.676536\n",
            "Epoch   7 | Loss: 0.669102\n",
            "Epoch   8 | Loss: 0.663319\n",
            "Epoch   9 | Loss: 0.659094\n",
            "Epoch  10 | Loss: 0.654320\n",
            "Epoch  11 | Loss: 0.650410\n",
            "Epoch  12 | Loss: 0.647628\n",
            "Epoch  13 | Loss: 0.644521\n",
            "Epoch  14 | Loss: 0.642770\n",
            "Epoch  15 | Loss: 0.640690\n",
            "Epoch  50 | Loss: 0.629733\n",
            "Epoch 100 | Loss: 0.627113\n",
            "Epoch 150 | Loss: 0.625282\n",
            "Epoch 200 | Loss: 0.624375\n",
            "Final test loss on held-out 1000 points: 0.626117\n",
            "Epoch   1 | Loss: 1.486490\n",
            "Epoch   2 | Loss: 0.632863\n",
            "Epoch   3 | Loss: 0.605672\n",
            "Epoch   4 | Loss: 0.555198\n",
            "Epoch   5 | Loss: 0.515476\n",
            "Epoch   6 | Loss: 0.473041\n",
            "Epoch   7 | Loss: 0.439392\n",
            "Epoch   8 | Loss: 0.399441\n",
            "Epoch   9 | Loss: 0.363331\n",
            "Epoch  10 | Loss: 0.322067\n",
            "Epoch  11 | Loss: 0.292385\n",
            "Epoch  12 | Loss: 0.262069\n",
            "Epoch  13 | Loss: 0.230909\n",
            "Epoch  14 | Loss: 0.206164\n",
            "Epoch  15 | Loss: 0.189872\n",
            "Converged at epoch 19\n",
            "Convergence broken at epoch 20\n",
            "Converged at epoch 21\n",
            "Epoch  50 | Loss: 0.107315\n",
            "Epoch 100 | Loss: 0.110093\n",
            "Epoch 150 | Loss: 0.104236\n",
            "Epoch 200 | Loss: 0.103829\n",
            "Final test loss on held-out 1000 points: 0.115493\n",
            "Epoch   1 | Total Loss: 2.279203 | MSE Loss: 2.194775\n",
            "Epoch   2 | Total Loss: 0.713808 | MSE Loss: 0.709814\n",
            "Epoch   3 | Total Loss: 0.651892 | MSE Loss: 0.639607\n",
            "Epoch   4 | Total Loss: 0.630327 | MSE Loss: 0.610685\n",
            "Epoch   5 | Total Loss: 0.611077 | MSE Loss: 0.585846\n",
            "Epoch   6 | Total Loss: 0.594952 | MSE Loss: 0.561645\n",
            "Epoch   7 | Total Loss: 0.599966 | MSE Loss: 0.561633\n",
            "Converged at epoch 7\n",
            "Epoch   8 | Total Loss: 0.575466 | MSE Loss: 0.530275\n",
            "Convergence broken at epoch 8\n",
            "Epoch   9 | Total Loss: 0.569041 | MSE Loss: 0.516674\n",
            "Epoch  10 | Total Loss: 0.561475 | MSE Loss: 0.504244\n",
            "Epoch  11 | Total Loss: 0.551940 | MSE Loss: 0.489480\n",
            "Epoch  12 | Total Loss: 0.548260 | MSE Loss: 0.480483\n",
            "Converged at epoch 12\n",
            "Epoch  13 | Total Loss: 0.537104 | MSE Loss: 0.460622\n",
            "Convergence broken at epoch 13\n",
            "Epoch  14 | Total Loss: 0.529883 | MSE Loss: 0.447229\n",
            "Epoch  15 | Total Loss: 0.523236 | MSE Loss: 0.433299\n",
            "Converged at epoch 19\n",
            "Convergence broken at epoch 20\n",
            "Converged at epoch 24\n",
            "Convergence broken at epoch 25\n",
            "Converged at epoch 26\n",
            "Convergence broken at epoch 27\n",
            "Converged at epoch 28\n",
            "Convergence broken at epoch 34\n",
            "Converged at epoch 35\n",
            "Epoch  50 | Total Loss: 0.387212 | MSE Loss: 0.208136\n",
            "Epoch 100 | Total Loss: 0.362318 | MSE Loss: 0.176803\n",
            "Epoch 150 | Total Loss: 0.347558 | MSE Loss: 0.164260\n",
            "Epoch 200 | Total Loss: 0.338578 | MSE Loss: 0.157157\n",
            "Final test loss on held-out 1000 points: 0.157461\n",
            "Avg gradient norm ratio (penalty/base): 0.767\n",
            "Avg grad cosine (base vs penalty): -0.590\n",
            "Avg directional dominance ratio: -2.067\n",
            "Epoch   1 | Loss: 14.418278\n",
            "Epoch   2 | Loss: 0.794752\n",
            "Epoch   3 | Loss: 0.660828\n",
            "Epoch   4 | Loss: 0.643617\n",
            "Epoch   5 | Loss: 0.640024\n",
            "Converged at epoch 5\n",
            "Epoch   6 | Loss: 0.637923\n",
            "Epoch   7 | Loss: 0.637643\n",
            "Epoch   8 | Loss: 0.636022\n",
            "Epoch   9 | Loss: 0.636190\n",
            "Epoch  10 | Loss: 0.635730\n",
            "Epoch  11 | Loss: 0.634924\n",
            "Epoch  12 | Loss: 0.634373\n",
            "Epoch  13 | Loss: 0.633928\n",
            "Epoch  14 | Loss: 0.633630\n",
            "Epoch  15 | Loss: 0.633447\n",
            "Epoch  50 | Loss: 0.630413\n",
            "Epoch 100 | Loss: 0.627315\n",
            "Epoch 150 | Loss: 0.625224\n",
            "Epoch 200 | Loss: 0.622601\n",
            "Final test loss on held-out 1000 points: 0.601317\n",
            "Epoch   1 | Loss: 1.094508\n",
            "Epoch   2 | Loss: 0.520873\n",
            "Epoch   3 | Loss: 0.481392\n",
            "Epoch   4 | Loss: 0.445793\n",
            "Epoch   5 | Loss: 0.412256\n",
            "Epoch   6 | Loss: 0.370586\n",
            "Epoch   7 | Loss: 0.339175\n",
            "Epoch   8 | Loss: 0.311537\n",
            "Epoch   9 | Loss: 0.275906\n",
            "Epoch  10 | Loss: 0.243490\n",
            "Epoch  11 | Loss: 0.228171\n",
            "Epoch  12 | Loss: 0.204197\n",
            "Epoch  13 | Loss: 0.184981\n",
            "Epoch  14 | Loss: 0.169276\n",
            "Epoch  15 | Loss: 0.156159\n",
            "Converged at epoch 17\n",
            "Convergence broken at epoch 18\n",
            "Converged at epoch 19\n",
            "Epoch  50 | Loss: 0.099293\n",
            "Epoch 100 | Loss: 0.091726\n",
            "Epoch 150 | Loss: 0.089158\n",
            "Epoch 200 | Loss: 0.088060\n",
            "Final test loss on held-out 1000 points: 0.102053\n",
            "Epoch   1 | Total Loss: 4.046558 | MSE Loss: 3.860185\n",
            "Epoch   2 | Total Loss: 2.491659 | MSE Loss: 2.491137\n",
            "Epoch   3 | Total Loss: 2.167639 | MSE Loss: 2.167412\n",
            "Epoch   4 | Total Loss: 1.994022 | MSE Loss: 1.993992\n",
            "Epoch   5 | Total Loss: 1.888890 | MSE Loss: 1.888828\n",
            "Epoch   6 | Total Loss: 1.818248 | MSE Loss: 1.818171\n",
            "Epoch   7 | Total Loss: 1.762434 | MSE Loss: 1.762162\n",
            "Epoch   8 | Total Loss: 1.715441 | MSE Loss: 1.715196\n",
            "Epoch   9 | Total Loss: 1.668349 | MSE Loss: 1.668189\n",
            "Epoch  10 | Total Loss: 1.622378 | MSE Loss: 1.622256\n",
            "Epoch  11 | Total Loss: 1.575007 | MSE Loss: 1.574864\n",
            "Epoch  12 | Total Loss: 1.528200 | MSE Loss: 1.528146\n",
            "Epoch  13 | Total Loss: 1.481603 | MSE Loss: 1.481539\n",
            "Epoch  14 | Total Loss: 1.436180 | MSE Loss: 1.436116\n",
            "Epoch  15 | Total Loss: 1.389959 | MSE Loss: 1.389876\n",
            "Epoch  50 | Total Loss: 0.624607 | MSE Loss: 0.594546\n",
            "Converged at epoch 52\n",
            "Convergence broken at epoch 53\n",
            "Converged at epoch 54\n",
            "Convergence broken at epoch 55\n",
            "Converged at epoch 56\n",
            "Convergence broken at epoch 57\n",
            "Converged at epoch 58\n",
            "Convergence broken at epoch 59\n",
            "Converged at epoch 60\n",
            "Convergence broken at epoch 68\n",
            "Converged at epoch 69\n",
            "Convergence broken at epoch 70\n",
            "Converged at epoch 71\n",
            "Convergence broken at epoch 76\n",
            "Converged at epoch 77\n",
            "Convergence broken at epoch 96\n",
            "Converged at epoch 97\n",
            "Epoch 100 | Total Loss: 0.396396 | MSE Loss: 0.357077\n",
            "Convergence broken at epoch 103\n",
            "Converged at epoch 104\n",
            "Convergence broken at epoch 107\n",
            "Converged at epoch 108\n",
            "Convergence broken at epoch 113\n",
            "Converged at epoch 114\n",
            "Convergence broken at epoch 142\n",
            "Converged at epoch 143\n",
            "Convergence broken at epoch 146\n",
            "Converged at epoch 147\n",
            "Epoch 150 | Total Loss: 0.329224 | MSE Loss: 0.300009\n",
            "Convergence broken at epoch 151\n",
            "Converged at epoch 152\n",
            "Convergence broken at epoch 153\n",
            "Converged at epoch 154\n",
            "Convergence broken at epoch 155\n",
            "Converged at epoch 156\n",
            "Convergence broken at epoch 174\n",
            "Converged at epoch 175\n",
            "Convergence broken at epoch 176\n",
            "Converged at epoch 177\n",
            "Convergence broken at epoch 179\n",
            "Converged at epoch 180\n",
            "Convergence broken at epoch 184\n",
            "Converged at epoch 185\n",
            "Convergence broken at epoch 186\n",
            "Converged at epoch 187\n",
            "Convergence broken at epoch 188\n",
            "Converged at epoch 189\n",
            "Epoch 200 | Total Loss: 0.273880 | MSE Loss: 0.257362\n",
            "Final test loss on held-out 1000 points: 0.330388\n",
            "Avg gradient norm ratio (penalty/base): 0.118\n",
            "Avg grad cosine (base vs penalty): -0.134\n",
            "Avg directional dominance ratio: 0.011\n",
            "Epoch   1 | Loss: 4.515017\n",
            "Epoch   2 | Loss: 1.126622\n",
            "Epoch   3 | Loss: 0.959574\n",
            "Epoch   4 | Loss: 0.859377\n",
            "Epoch   5 | Loss: 0.797201\n",
            "Epoch   6 | Loss: 0.752424\n",
            "Epoch   7 | Loss: 0.723270\n",
            "Epoch   8 | Loss: 0.701494\n",
            "Epoch   9 | Loss: 0.686501\n",
            "Epoch  10 | Loss: 0.674233\n",
            "Epoch  11 | Loss: 0.667640\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Loss: 0.661006\n",
            "Epoch  13 | Loss: 0.655406\n",
            "Epoch  14 | Loss: 0.652819\n",
            "Epoch  15 | Loss: 0.648913\n",
            "Epoch  50 | Loss: 0.637843\n",
            "Epoch 100 | Loss: 0.637014\n",
            "Epoch 150 | Loss: 0.636468\n",
            "Epoch 200 | Loss: 0.639480\n",
            "Final test loss on held-out 1000 points: 0.695834\n",
            "Epoch   1 | Loss: 1.705816\n",
            "Epoch   2 | Loss: 0.661971\n",
            "Epoch   3 | Loss: 0.609661\n",
            "Epoch   4 | Loss: 0.612781\n",
            "Converged at epoch 4\n",
            "Epoch   5 | Loss: 0.595454\n",
            "Convergence broken at epoch 5\n",
            "Epoch   6 | Loss: 0.598239\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Loss: 0.598882\n",
            "Epoch   8 | Loss: 0.593182\n",
            "Epoch   9 | Loss: 0.591210\n",
            "Epoch  10 | Loss: 0.590074\n",
            "Epoch  11 | Loss: 0.590497\n",
            "Epoch  12 | Loss: 0.592437\n",
            "Epoch  13 | Loss: 0.587756\n",
            "Epoch  14 | Loss: 0.589883\n",
            "Epoch  15 | Loss: 0.589952\n",
            "Epoch  50 | Loss: 0.585127\n",
            "Epoch 100 | Loss: 0.575168\n",
            "Epoch 150 | Loss: 0.578798\n",
            "Epoch 200 | Loss: 0.579325\n",
            "Final test loss on held-out 1000 points: 0.603163\n",
            "Epoch   1 | Total Loss: 1.045704 | MSE Loss: 1.002006\n",
            "Epoch   2 | Total Loss: 0.742562 | MSE Loss: 0.742562\n",
            "Epoch   3 | Total Loss: 0.664422 | MSE Loss: 0.664422\n",
            "Epoch   4 | Total Loss: 0.646571 | MSE Loss: 0.646571\n",
            "Epoch   5 | Total Loss: 0.624718 | MSE Loss: 0.624718\n",
            "Epoch   6 | Total Loss: 0.622002 | MSE Loss: 0.622002\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Total Loss: 0.617822 | MSE Loss: 0.617822\n",
            "Epoch   8 | Total Loss: 0.615858 | MSE Loss: 0.615858\n",
            "Epoch   9 | Total Loss: 0.613268 | MSE Loss: 0.613268\n",
            "Epoch  10 | Total Loss: 0.616501 | MSE Loss: 0.616501\n",
            "Epoch  11 | Total Loss: 0.602483 | MSE Loss: 0.602483\n",
            "Convergence broken at epoch 11\n",
            "Epoch  12 | Total Loss: 0.600259 | MSE Loss: 0.600259\n",
            "Converged at epoch 12\n",
            "Epoch  13 | Total Loss: 0.612809 | MSE Loss: 0.612809\n",
            "Epoch  14 | Total Loss: 0.602810 | MSE Loss: 0.602810\n",
            "Epoch  15 | Total Loss: 0.606316 | MSE Loss: 0.606316\n",
            "Convergence broken at epoch 39\n",
            "Converged at epoch 40\n",
            "Epoch  50 | Total Loss: 0.593088 | MSE Loss: 0.593088\n",
            "Convergence broken at epoch 50\n",
            "Converged at epoch 51\n",
            "Convergence broken at epoch 69\n",
            "Converged at epoch 70\n",
            "Epoch 100 | Total Loss: 0.611691 | MSE Loss: 0.611691\n",
            "Convergence broken at epoch 112\n",
            "Converged at epoch 113\n",
            "Epoch 150 | Total Loss: 0.600940 | MSE Loss: 0.600940\n",
            "Epoch 200 | Total Loss: 0.605435 | MSE Loss: 0.605435\n",
            "Final test loss on held-out 1000 points: 0.627473\n",
            "Avg gradient norm ratio (penalty/base): 0.001\n",
            "Avg grad cosine (base vs penalty): 0.001\n",
            "Avg directional dominance ratio: 0.001\n",
            "Epoch   1 | Loss: 18.608786\n",
            "Epoch   2 | Loss: 0.912375\n",
            "Epoch   3 | Loss: 0.677379\n",
            "Epoch   4 | Loss: 0.633662\n",
            "Epoch   5 | Loss: 0.622266\n",
            "Epoch   6 | Loss: 0.616990\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Loss: 0.614866\n",
            "Epoch   8 | Loss: 0.611816\n",
            "Epoch   9 | Loss: 0.610315\n",
            "Epoch  10 | Loss: 0.608044\n",
            "Epoch  11 | Loss: 0.607050\n",
            "Epoch  12 | Loss: 0.605620\n",
            "Epoch  13 | Loss: 0.604270\n",
            "Epoch  14 | Loss: 0.603736\n",
            "Epoch  15 | Loss: 0.602763\n",
            "Epoch  50 | Loss: 0.597826\n",
            "Epoch 100 | Loss: 0.597704\n",
            "Epoch 150 | Loss: 0.597134\n",
            "Epoch 200 | Loss: 0.597718\n",
            "Final test loss on held-out 1000 points: 0.628210\n",
            "Epoch   1 | Loss: 3.864959\n",
            "Epoch   2 | Loss: 0.847440\n",
            "Epoch   3 | Loss: 0.764386\n",
            "Epoch   4 | Loss: 0.725833\n",
            "Epoch   5 | Loss: 0.697505\n",
            "Epoch   6 | Loss: 0.675856\n",
            "Epoch   7 | Loss: 0.663499\n",
            "Epoch   8 | Loss: 0.656241\n",
            "Converged at epoch 8\n",
            "Epoch   9 | Loss: 0.668001\n",
            "Epoch  10 | Loss: 0.647744\n",
            "Convergence broken at epoch 10\n",
            "Epoch  11 | Loss: 0.642192\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Loss: 0.638225\n",
            "Epoch  13 | Loss: 0.637874\n",
            "Epoch  14 | Loss: 0.637251\n",
            "Epoch  15 | Loss: 0.638387\n",
            "Convergence broken at epoch 42\n",
            "Converged at epoch 43\n",
            "Epoch  50 | Loss: 0.624467\n",
            "Convergence broken at epoch 80\n",
            "Converged at epoch 81\n",
            "Epoch 100 | Loss: 0.599095\n",
            "Convergence broken at epoch 108\n",
            "Converged at epoch 109\n",
            "Convergence broken at epoch 135\n",
            "Converged at epoch 136\n",
            "Epoch 150 | Loss: 0.593507\n",
            "Convergence broken at epoch 158\n",
            "Converged at epoch 159\n",
            "Convergence broken at epoch 163\n",
            "Converged at epoch 164\n",
            "Convergence broken at epoch 177\n",
            "Converged at epoch 178\n",
            "Convergence broken at epoch 196\n",
            "Converged at epoch 197\n",
            "Epoch 200 | Loss: 0.586672\n",
            "Final test loss on held-out 1000 points: 0.575859\n",
            "Epoch   1 | Total Loss: 0.865656 | MSE Loss: 0.865656\n",
            "Epoch   2 | Total Loss: 0.763395 | MSE Loss: 0.763395\n",
            "Epoch   3 | Total Loss: 0.721329 | MSE Loss: 0.721329\n",
            "Epoch   4 | Total Loss: 0.692546 | MSE Loss: 0.692546\n",
            "Epoch   5 | Total Loss: 0.673279 | MSE Loss: 0.673279\n",
            "Epoch   6 | Total Loss: 0.664081 | MSE Loss: 0.664081\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Total Loss: 0.654994 | MSE Loss: 0.654994\n",
            "Epoch   8 | Total Loss: 0.649267 | MSE Loss: 0.649267\n",
            "Epoch   9 | Total Loss: 0.653968 | MSE Loss: 0.653968\n",
            "Epoch  10 | Total Loss: 0.643011 | MSE Loss: 0.643011\n",
            "Convergence broken at epoch 10\n",
            "Epoch  11 | Total Loss: 0.645783 | MSE Loss: 0.645783\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Total Loss: 0.643089 | MSE Loss: 0.643089\n",
            "Epoch  13 | Total Loss: 0.639695 | MSE Loss: 0.639695\n",
            "Epoch  14 | Total Loss: 0.634535 | MSE Loss: 0.634535\n",
            "Epoch  15 | Total Loss: 0.630422 | MSE Loss: 0.630422\n",
            "Convergence broken at epoch 17\n",
            "Converged at epoch 36\n",
            "Convergence broken at epoch 37\n",
            "Converged at epoch 38\n",
            "Epoch  50 | Total Loss: 0.109233 | MSE Loss: 0.109233\n",
            "Convergence broken at epoch 87\n",
            "Converged at epoch 88\n",
            "Epoch 100 | Total Loss: 0.099291 | MSE Loss: 0.099291\n",
            "Epoch 150 | Total Loss: 0.104339 | MSE Loss: 0.104339\n",
            "Epoch 200 | Total Loss: 0.101531 | MSE Loss: 0.101531\n",
            "Final test loss on held-out 1000 points: 0.112206\n",
            "Avg gradient norm ratio (penalty/base): 0.000\n",
            "Avg grad cosine (base vs penalty): 0.000\n",
            "Avg directional dominance ratio: 0.000\n",
            "Epoch   1 | Loss: 8.170788\n",
            "Epoch   2 | Loss: 0.716524\n",
            "Epoch   3 | Loss: 0.612980\n",
            "Epoch   4 | Loss: 0.597992\n",
            "Epoch   5 | Loss: 0.594161\n",
            "Converged at epoch 5\n",
            "Epoch   6 | Loss: 0.591694\n",
            "Epoch   7 | Loss: 0.589777\n",
            "Epoch   8 | Loss: 0.588448\n",
            "Epoch   9 | Loss: 0.588124\n",
            "Epoch  10 | Loss: 0.588694\n",
            "Epoch  11 | Loss: 0.587460\n",
            "Epoch  12 | Loss: 0.586711\n",
            "Epoch  13 | Loss: 0.587030\n",
            "Epoch  14 | Loss: 0.586857\n",
            "Epoch  15 | Loss: 0.585569\n",
            "Epoch  50 | Loss: 0.581044\n",
            "Epoch 100 | Loss: 0.579070\n",
            "Epoch 150 | Loss: 0.577378\n",
            "Epoch 200 | Loss: 0.577953\n",
            "Final test loss on held-out 1000 points: 0.568676\n",
            "Epoch   1 | Loss: 2.989961\n",
            "Epoch   2 | Loss: 1.447677\n",
            "Epoch   3 | Loss: 0.875967\n",
            "Epoch   4 | Loss: 0.810496\n",
            "Epoch   5 | Loss: 0.764488\n",
            "Epoch   6 | Loss: 0.731326\n",
            "Epoch   7 | Loss: 0.704314\n",
            "Epoch   8 | Loss: 0.686529\n",
            "Epoch   9 | Loss: 0.673252\n",
            "Epoch  10 | Loss: 0.663016\n",
            "Epoch  11 | Loss: 0.655441\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Loss: 0.656191\n",
            "Epoch  13 | Loss: 0.647536\n",
            "Epoch  14 | Loss: 0.645342\n",
            "Epoch  15 | Loss: 0.643776\n",
            "Convergence broken at epoch 20\n",
            "Converged at epoch 21\n",
            "Epoch  50 | Loss: 0.640031\n",
            "Convergence broken at epoch 66\n",
            "Converged at epoch 67\n",
            "Epoch 100 | Loss: 0.627344\n",
            "Convergence broken at epoch 137\n",
            "Converged at epoch 138\n",
            "Epoch 150 | Loss: 0.611815\n",
            "Epoch 200 | Loss: 0.620272\n",
            "Final test loss on held-out 1000 points: 0.606538\n",
            "Epoch   1 | Total Loss: 4.079609 | MSE Loss: 3.871414\n",
            "Epoch   2 | Total Loss: 3.056001 | MSE Loss: 2.867334\n",
            "Epoch   3 | Total Loss: 2.191166 | MSE Loss: 2.054646\n",
            "Epoch   4 | Total Loss: 1.316994 | MSE Loss: 1.289131\n",
            "Epoch   5 | Total Loss: 0.964729 | MSE Loss: 0.956836\n",
            "Epoch   6 | Total Loss: 0.809134 | MSE Loss: 0.809089\n",
            "Epoch   7 | Total Loss: 0.741005 | MSE Loss: 0.741005\n",
            "Epoch   8 | Total Loss: 0.700133 | MSE Loss: 0.700133\n",
            "Epoch   9 | Total Loss: 0.679248 | MSE Loss: 0.679248\n",
            "Epoch  10 | Total Loss: 0.663931 | MSE Loss: 0.663931\n",
            "Epoch  11 | Total Loss: 0.658933 | MSE Loss: 0.658933\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Total Loss: 0.648818 | MSE Loss: 0.648818\n",
            "Convergence broken at epoch 12\n",
            "Epoch  13 | Total Loss: 0.641190 | MSE Loss: 0.641190\n",
            "Converged at epoch 13\n",
            "Epoch  14 | Total Loss: 0.635457 | MSE Loss: 0.635457\n",
            "Epoch  15 | Total Loss: 0.632760 | MSE Loss: 0.632760\n",
            "Epoch  50 | Total Loss: 0.619401 | MSE Loss: 0.619401\n",
            "Convergence broken at epoch 94\n",
            "Converged at epoch 95\n",
            "Epoch 100 | Total Loss: 0.610992 | MSE Loss: 0.610992\n",
            "Convergence broken at epoch 105\n",
            "Converged at epoch 106\n",
            "Convergence broken at epoch 130\n",
            "Converged at epoch 131\n",
            "Epoch 150 | Total Loss: 0.596813 | MSE Loss: 0.596813\n",
            "Convergence broken at epoch 152\n",
            "Converged at epoch 153\n",
            "Convergence broken at epoch 190\n",
            "Converged at epoch 191\n",
            "Epoch 200 | Total Loss: 0.599861 | MSE Loss: 0.599861\n",
            "Final test loss on held-out 1000 points: 0.589463\n",
            "Avg gradient norm ratio (penalty/base): 0.008\n",
            "Avg grad cosine (base vs penalty): 0.001\n",
            "Avg directional dominance ratio: 0.004\n",
            "Epoch   1 | Loss: 6.516996\n",
            "Epoch   2 | Loss: 1.276105\n",
            "Epoch   3 | Loss: 0.829562\n",
            "Epoch   4 | Loss: 0.718807\n",
            "Epoch   5 | Loss: 0.679004\n",
            "Epoch   6 | Loss: 0.657963\n",
            "Epoch   7 | Loss: 0.644226\n",
            "Epoch   8 | Loss: 0.632686\n",
            "Epoch   9 | Loss: 0.623591\n",
            "Converged at epoch 9\n",
            "Epoch  10 | Loss: 0.616325\n",
            "Epoch  11 | Loss: 0.610293\n",
            "Epoch  12 | Loss: 0.605175\n",
            "Epoch  13 | Loss: 0.601347\n",
            "Epoch  14 | Loss: 0.597467\n",
            "Epoch  15 | Loss: 0.594569\n",
            "Epoch  50 | Loss: 0.578528\n",
            "Epoch 100 | Loss: 0.571193\n",
            "Epoch 150 | Loss: 0.565739\n",
            "Epoch 200 | Loss: 0.562285\n",
            "Final test loss on held-out 1000 points: 0.559926\n",
            "Epoch   1 | Loss: 2.621611\n",
            "Epoch   2 | Loss: 0.760015\n",
            "Epoch   3 | Loss: 0.638211\n",
            "Epoch   4 | Loss: 0.636036\n",
            "Converged at epoch 4\n",
            "Epoch   5 | Loss: 0.618725\n",
            "Convergence broken at epoch 5\n",
            "Epoch   6 | Loss: 0.610877\n",
            "Converged at epoch 6\n",
            "Epoch   7 | Loss: 0.610981\n",
            "Epoch   8 | Loss: 0.616968\n",
            "Epoch   9 | Loss: 0.603396\n",
            "Convergence broken at epoch 9\n",
            "Epoch  10 | Loss: 0.616745\n",
            "Converged at epoch 10\n",
            "Epoch  11 | Loss: 0.605484\n",
            "Convergence broken at epoch 11\n",
            "Epoch  12 | Loss: 0.608637\n",
            "Converged at epoch 12\n",
            "Epoch  13 | Loss: 0.605251\n",
            "Epoch  14 | Loss: 0.610797\n",
            "Epoch  15 | Loss: 0.602515\n",
            "Epoch  50 | Loss: 0.601647\n",
            "Convergence broken at epoch 65\n",
            "Converged at epoch 66\n",
            "Convergence broken at epoch 92\n",
            "Converged at epoch 93\n",
            "Epoch 100 | Loss: 0.593488\n",
            "Epoch 150 | Loss: 0.595708\n",
            "Convergence broken at epoch 164\n",
            "Converged at epoch 165\n",
            "Epoch 200 | Loss: 0.599121\n",
            "Final test loss on held-out 1000 points: 0.632768\n",
            "Epoch   1 | Total Loss: 0.734198 | MSE Loss: 0.734198\n",
            "Epoch   2 | Total Loss: 0.676676 | MSE Loss: 0.676676\n",
            "Epoch   3 | Total Loss: 0.627756 | MSE Loss: 0.627756\n",
            "Epoch   4 | Total Loss: 0.588533 | MSE Loss: 0.588533\n",
            "Epoch   5 | Total Loss: 0.549124 | MSE Loss: 0.549124\n",
            "Epoch   6 | Total Loss: 0.510605 | MSE Loss: 0.510605\n",
            "Epoch   7 | Total Loss: 0.474599 | MSE Loss: 0.474599\n",
            "Epoch   8 | Total Loss: 0.442350 | MSE Loss: 0.442350\n",
            "Epoch   9 | Total Loss: 0.409471 | MSE Loss: 0.409471\n",
            "Epoch  10 | Total Loss: 0.375733 | MSE Loss: 0.375733\n",
            "Epoch  11 | Total Loss: 0.342918 | MSE Loss: 0.342918\n",
            "Epoch  12 | Total Loss: 0.311923 | MSE Loss: 0.311923\n",
            "Epoch  13 | Total Loss: 0.282491 | MSE Loss: 0.282491\n",
            "Epoch  14 | Total Loss: 0.252952 | MSE Loss: 0.252952\n",
            "Epoch  15 | Total Loss: 0.230016 | MSE Loss: 0.230016\n",
            "Converged at epoch 20\n",
            "Epoch  50 | Total Loss: 0.098421 | MSE Loss: 0.098421\n",
            "Epoch 100 | Total Loss: 0.088398 | MSE Loss: 0.088398\n",
            "Epoch 150 | Total Loss: 0.085832 | MSE Loss: 0.085832\n",
            "Epoch 200 | Total Loss: 0.087126 | MSE Loss: 0.085178\n",
            "Final test loss on held-out 1000 points: 0.098144\n",
            "Avg gradient norm ratio (penalty/base): 0.002\n",
            "Avg grad cosine (base vs penalty): -0.002\n",
            "Avg directional dominance ratio: -0.000\n",
            "Epoch   1 | Loss: 7.975908\n",
            "Epoch   2 | Loss: 1.042281\n",
            "Epoch   3 | Loss: 0.860067\n",
            "Epoch   4 | Loss: 0.808556\n",
            "Epoch   5 | Loss: 0.778412\n",
            "Epoch   6 | Loss: 0.754892\n",
            "Epoch   7 | Loss: 0.736148\n",
            "Epoch   8 | Loss: 0.722052\n",
            "Epoch   9 | Loss: 0.710145\n",
            "Epoch  10 | Loss: 0.699228\n",
            "Epoch  11 | Loss: 0.689501\n",
            "Converged at epoch 11\n",
            "Epoch  12 | Loss: 0.681997\n",
            "Epoch  13 | Loss: 0.674206\n",
            "Epoch  14 | Loss: 0.666853\n",
            "Epoch  15 | Loss: 0.661296\n",
            "Epoch  50 | Loss: 0.569520\n",
            "Epoch 100 | Loss: 0.556890\n",
            "Epoch 150 | Loss: 0.550301\n",
            "Epoch 200 | Loss: 0.543140\n",
            "Final test loss on held-out 1000 points: 0.558053\n",
            "Epoch   1 | Loss: 3.072849\n",
            "Epoch   2 | Loss: 2.091066\n",
            "Epoch   3 | Loss: 1.158007\n",
            "Epoch   4 | Loss: 0.854210\n",
            "Epoch   5 | Loss: 0.719748\n",
            "Epoch   6 | Loss: 0.585325\n",
            "Epoch   7 | Loss: 0.519241\n",
            "Epoch   8 | Loss: 0.474970\n",
            "Epoch   9 | Loss: 0.443012\n",
            "Epoch  10 | Loss: 0.411360\n",
            "Epoch  11 | Loss: 0.387473\n",
            "Epoch  12 | Loss: 0.360196\n",
            "Epoch  13 | Loss: 0.336222\n",
            "Epoch  14 | Loss: 0.310007\n",
            "Epoch  15 | Loss: 0.294734\n",
            "Converged at epoch 23\n",
            "Convergence broken at epoch 27\n",
            "Converged at epoch 28\n",
            "Epoch  50 | Loss: 0.087194\n",
            "Epoch 100 | Loss: 0.081101\n",
            "Epoch 150 | Loss: 0.079837\n",
            "Epoch 200 | Loss: 0.082531\n",
            "Final test loss on held-out 1000 points: 0.121269\n",
            "Epoch   1 | Total Loss: 1.466983 | MSE Loss: 1.466983\n",
            "Epoch   2 | Total Loss: 0.559647 | MSE Loss: 0.559647\n",
            "Epoch   3 | Total Loss: 0.550496 | MSE Loss: 0.550496\n",
            "Converged at epoch 3\n",
            "Epoch   4 | Total Loss: 0.549735 | MSE Loss: 0.549735\n",
            "Epoch   5 | Total Loss: 0.551066 | MSE Loss: 0.551066\n",
            "Epoch   6 | Total Loss: 0.548038 | MSE Loss: 0.548038\n",
            "Epoch   7 | Total Loss: 0.546406 | MSE Loss: 0.546406\n",
            "Epoch   8 | Total Loss: 0.553282 | MSE Loss: 0.553282\n",
            "Epoch   9 | Total Loss: 0.546165 | MSE Loss: 0.546165\n",
            "Epoch  10 | Total Loss: 0.550674 | MSE Loss: 0.550674\n",
            "Epoch  11 | Total Loss: 0.551026 | MSE Loss: 0.551026\n",
            "Epoch  12 | Total Loss: 0.548720 | MSE Loss: 0.548720\n",
            "Epoch  13 | Total Loss: 0.550718 | MSE Loss: 0.550718\n",
            "Epoch  14 | Total Loss: 0.549782 | MSE Loss: 0.549782\n",
            "Epoch  15 | Total Loss: 0.549486 | MSE Loss: 0.549486\n",
            "Epoch  50 | Total Loss: 0.552236 | MSE Loss: 0.552236\n",
            "Epoch 100 | Total Loss: 0.545205 | MSE Loss: 0.545205\n",
            "Epoch 150 | Total Loss: 0.546875 | MSE Loss: 0.546875\n",
            "Epoch 200 | Total Loss: 0.551193 | MSE Loss: 0.551193\n",
            "Final test loss on held-out 1000 points: 0.688021\n",
            "Avg gradient norm ratio (penalty/base): 0.000\n",
            "Avg grad cosine (base vs penalty): 0.000\n",
            "Avg directional dominance ratio: 0.000\n",
            "Epoch   1 | Loss: 7.773203\n",
            "Epoch   2 | Loss: 1.121181\n",
            "Epoch   3 | Loss: 0.755047\n",
            "Epoch   4 | Loss: 0.664431\n",
            "Epoch   5 | Loss: 0.629321\n",
            "Epoch   6 | Loss: 0.609036\n",
            "Epoch   7 | Loss: 0.595048\n",
            "Epoch   8 | Loss: 0.583005\n",
            "Epoch   9 | Loss: 0.573507\n",
            "Converged at epoch 9\n",
            "Epoch  10 | Loss: 0.564830\n",
            "Epoch  11 | Loss: 0.558560\n",
            "Epoch  12 | Loss: 0.553191\n",
            "Epoch  13 | Loss: 0.549005\n",
            "Epoch  14 | Loss: 0.544634\n",
            "Epoch  15 | Loss: 0.541944\n",
            "Epoch  50 | Loss: 0.522027\n",
            "Epoch 100 | Loss: 0.513324\n",
            "Epoch 150 | Loss: 0.510822\n",
            "Epoch 200 | Loss: 0.508850\n",
            "Final test loss on held-out 1000 points: 0.635550\n",
            "Average initial weight value: tensor(0.0269)\n",
            "Average final weight (Base): tensor(0.1135)\n",
            "Average final weight (Loss): tensor(0.1438)\n",
            "Average final weight (Arch): tensor(-0.4616)\n",
            "Average distance (Base): tensor(0.2691)\n",
            "Average distance (Loss): tensor(0.6377)\n",
            "Average distance (Arch): tensor(0.7848)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base)\n",
        "print(loss)\n",
        "print(arch)"
      ],
      "metadata": {
        "id": "ZAW6Pe6f21CC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "382810a9-638e-4b4f-e9fb-9f57a3ddc8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'base' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3550665988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del loss[9]\n",
        "# del loss[7]\n",
        "# del loss[4]\n",
        "# del loss[1]\n",
        "# del loss[0]\n",
        "\n",
        "# del base[9]\n",
        "# del base[7]\n",
        "# del base[4]\n",
        "# del base[1]\n",
        "# del base[0]\n",
        "\n",
        "# del arch[9]\n",
        "# del arch[7]\n",
        "# del arch[4]\n",
        "# del arch[1]\n",
        "# del arch[0]"
      ],
      "metadata": {
        "id": "Ip5QJ42XDxKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base = [(a, b, c, d, g) for (a, b, c, d, _, g) in base]\n",
        "# loss = [(a, b, c, d, g) for (a, b, c, d, _, g) in loss]\n",
        "# arch = [(a, b, c, d, g) for (a, b, c, d, _, g) in arch]"
      ],
      "metadata": {
        "id": "39MESIm_EnXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base = [sum(x) / len(x) for x in zip(*base)]\n",
        "# loss = [sum(x) / len(x) for x in zip(*loss)]\n",
        "# arch = [sum(x) / len(x) for x in zip(*arch)]"
      ],
      "metadata": {
        "id": "Ba2f-HDu572p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base"
      ],
      "metadata": {
        "id": "66XwWr406TmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "id": "eOnXXNqn6ULb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arch"
      ],
      "metadata": {
        "id": "EVEVWoiu6Vly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_trajectories = base[-1][-1]\n",
        "loss_trajectories = loss[-1][-1]"
      ],
      "metadata": {
        "id": "FCtFl5ut46dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_2d_loss_grid(example_mono_x, example_mono_y, baseline_nn, trajectory=base_trajectories,  savepath='baseline.png')\n",
        "visualise_2d_loss_grid(example_mono_x, example_mono_y, loss_nn, trajectory=loss_trajectories, savepath='loss.png')\n",
        "# visualise_2d_loss_grid(example_mono_x, example_mono_y, arch_nn, savepath='arch.png')"
      ],
      "metadata": {
        "id": "L5sU1zPqeTn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vz2FvBV46tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise_feasible_region_mono(example_mono_x, example_mono_y, baseline_nn, savepath='baseline_f.png')\n",
        "# visualise_feasible_region_mono(example_mono_x, example_mono_y, loss_nn, savepath='loss_f.png')\n",
        "# visualise_feasible_region_mono(example_mono_x, example_mono_y, arch_nn, savepath='arch_f.png')"
      ],
      "metadata": {
        "id": "hSnZdYiCeI7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clone_weights(baseline_nn))\n",
        "print(clone_weights(loss_nn))\n",
        "print(clone_weights(arch_nn))"
      ],
      "metadata": {
        "id": "sQaxK-spjgUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLLHN1Z5eWXcnI+N31B+T+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}